{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"asl_computer_vision_reader.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNL4NWUnJ0RxpRpDyE8VGuB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Live Sign Language Translator\n","This notebook will take video input from the user and transform it into a list of photos. It will then find what the asl equiv is to the inputted hand sign "],"metadata":{"id":"YfsPXgkxFgnO"}},{"cell_type":"code","source":["import numpy\n","import cv2\n","import mediapipe as mp\n","cap = cv2.VideoCapture(0)\n","mp_hands = mp.solutions.hands\n","hands = mp_hands.Hands()\n","mp_draw = mp.solutions.drawing_utils\n","\n","help(hands)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUq2cSlLF7h1","executionInfo":{"status":"ok","timestamp":1661889511695,"user_tz":420,"elapsed":174,"user":{"displayName":"Ashwin Tyagi","userId":"01848559523199568635"}},"outputId":"adbe4024-1796-4cd9-a590-9ebb36283967"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on Hands in module mediapipe.python.solutions.hands object:\n","\n","class Hands(mediapipe.python.solution_base.SolutionBase)\n"," |  Hands(static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n"," |  \n"," |  MediaPipe Hands.\n"," |  \n"," |  MediaPipe Hands processes an RGB image and returns the hand landmarks and\n"," |  handedness (left v.s. right hand) of each detected hand.\n"," |  \n"," |  Note that it determines handedness assuming the input image is mirrored,\n"," |  i.e., taken with a front-facing/selfie camera (\n"," |  https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped\n"," |  horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)\n"," |  to flip the image first for a correct handedness output.\n"," |  \n"," |  Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for\n"," |  usage examples.\n"," |  \n"," |  Method resolution order:\n"," |      Hands\n"," |      mediapipe.python.solution_base.SolutionBase\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, static_image_mode=False, max_num_hands=2, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n"," |      Initializes a MediaPipe Hand object.\n"," |      \n"," |      Args:\n"," |        static_image_mode: Whether to treat the input images as a batch of static\n"," |          and possibly unrelated images, or a video stream. See details in\n"," |          https://solutions.mediapipe.dev/hands#static_image_mode.\n"," |        max_num_hands: Maximum number of hands to detect. See details in\n"," |          https://solutions.mediapipe.dev/hands#max_num_hands.\n"," |        model_complexity: Complexity of the hand landmark model: 0 or 1.\n"," |          Landmark accuracy as well as inference latency generally go up with the\n"," |          model complexity. See details in\n"," |          https://solutions.mediapipe.dev/hands#model_complexity.\n"," |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand\n"," |          detection to be considered successful. See details in\n"," |          https://solutions.mediapipe.dev/hands#min_detection_confidence.\n"," |        min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n"," |          hand landmarks to be considered tracked successfully. See details in\n"," |          https://solutions.mediapipe.dev/hands#min_tracking_confidence.\n"," |  \n"," |  process(self, image: numpy.ndarray) -> <class 'NamedTuple'>\n"," |      Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\n"," |      \n"," |      Args:\n"," |        image: An RGB image represented as a numpy ndarray.\n"," |      \n"," |      Raises:\n"," |        RuntimeError: If the underlying graph throws any error.\n"," |        ValueError: If the input image is not three channel RGB.\n"," |      \n"," |      Returns:\n"," |        A NamedTuple object with the following fields:\n"," |          1) a \"multi_hand_landmarks\" field that contains the hand landmarks on\n"," |             each detected hand.\n"," |          2) a \"multi_hand_world_landmarks\" field that contains the hand landmarks\n"," |             on each detected hand in real-world 3D coordinates that are in meters\n"," |             with the origin at the hand's approximate geometric center.\n"," |          3) a \"multi_handedness\" field that contains the handedness (left v.s.\n"," |             right hand) of the detected hand.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n"," |  \n"," |  __enter__(self)\n"," |      A \"with\" statement support.\n"," |  \n"," |  __exit__(self, exc_type, exc_val, exc_tb)\n"," |      Closes all the input sources and the graph.\n"," |  \n"," |  close(self) -> None\n"," |      Closes all the input sources and the graph.\n"," |  \n"," |  create_graph_options(self, options_message: google.protobuf.message.Message, values: Mapping[str, Any]) -> google.protobuf.message.Message\n"," |      Sets protobuf field values.\n"," |      \n"," |      Args:\n"," |        options_message: the options protobuf message.\n"," |        values: field value pairs, where each field may be a \".\" separated path.\n"," |      \n"," |      Returns:\n"," |        the options protobuf message.\n"," |  \n"," |  reset(self) -> None\n"," |      Resets the graph for another run.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}]},{"cell_type":"markdown","source":["#### Capture Image input and proccess it"],"metadata":{"id":"7ovTBf72K-P9"}},{"cell_type":"code","source":["while True:\n","    success, image = cap.read()\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    results = hands.process(image_rgb)\n","\n","    if results.multi_hands_landmarks:\n","        for hand_lms in results.multi_hands_landmarks:\n","            for id, lm, in enumerate(hand_lms.landmark):\n","                h, w, c = image.shape\n","                cx, cy = int(lm.x * w), int(lm.y * h)\n","\n","                if id == 20:\n","                    cv2.circle(image, (cx, cy), 25, (255, 0, 255), cv2.FILLED)\n","\n","                mp_draw.draw_landmarks(image, hand_lms, mp_hands.HAND_CONNECTIONS)\n","            cv2.imshow(\"Output\", image)\n","            cv2.waitKey(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"mYKd0xV8K6RB","executionInfo":{"status":"error","timestamp":1661887221506,"user_tz":420,"elapsed":406,"user":{"displayName":"Ashwin Tyagi","userId":"01848559523199568635"}},"outputId":"a7d874a9-e678-4401-9eb9-df1190f107ca"},"execution_count":11,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-7ce68258fe33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]}]}